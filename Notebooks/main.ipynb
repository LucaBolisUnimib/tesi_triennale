{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054b9859-b4b1-43f9-bb00-19b74a8cb02f",
   "metadata": {},
   "source": [
    "# IMPLEMENTAZIONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f14cf5cc-4f3a-48df-8ed3-ca1cac49b2c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym Version = 0.29.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "print(f\"Gym Version = {gym.__version__}\")\n",
    "\n",
    "import AddictiveReward\n",
    "\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "from numba.experimental import jitclass\n",
    "\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics\n",
    "from gymnasium import spaces\n",
    "import random\n",
    "from numpy.random import choice\n",
    "#from tqdm.notebook import tqdm\n",
    "# from tqdm import tqdm, tqdm_notebook\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "\n",
    "#serializer\n",
    "from EnvSerializer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2795f3-c353-4f36-bc5f-bf07b1528143",
   "metadata": {},
   "source": [
    "## IMPLEMENTAZIONE AMBIENTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d7266b-caf0-4825-8180-2c70311d9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('AddictiveEnv_Avanzato')\n",
    "env = gym.make('AddictiveEnv_Semplificato')\n",
    "# env = gym.make('AddictiveEnv_Raffinato')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac74da00-11ed-473f-b7a8-f41d530caa73",
   "metadata": {},
   "source": [
    "## AGENTE MF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b0ad8-9e64-4a24-9566-a84053dbca11",
   "metadata": {},
   "source": [
    "### IMPLEMENTAZIONE AGENTE MF\n",
    "L'agente MF basato su Q-Learning volto a stimare il valore di una Q(s, a) ottimale (dalla quale poi estrarre la politica ottimale) in un ambiente senza avere conoscenza diretta del modello di transizione di stato o delle ricompense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af7f16cb-f328-43c1-ac20-b7837a7a4ac0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MFLearningAgent:\n",
    "    def __init__(self, \n",
    "                 learning_rate: float,  \n",
    "                 initial_epsilon: float, \n",
    "                 epsilon_decay: float,  \n",
    "                 final_epsilon: float, \n",
    "                 discount_factor: float = 0.9):\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        \n",
    "        # Initialize Q-values table\n",
    "        self.q_values = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.training_error = []  # Track training error during training\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Choose action using epsilon-greedy strategy\n",
    "        if random.uniform(0, 1) < self.epsilon: # epsilon greedy \n",
    "            return env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            return np.argmax(self.q_values[state]) # Exploit learned values\n",
    "\n",
    "    def update(self, obs: int, action: int, reward: float, terminated: bool, next_obs: int):\n",
    "        # Update Q-values using Q-learning equation\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (reward + self.discount_factor * future_q_value - self.q_values[obs][action])\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "        \n",
    "    def decay_epsilon(self):\n",
    "       # Decay epsilon value\n",
    "       self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d502ef63-efad-4a6a-be72-e5c3a59afe70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## AGENTE MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789785c-2abb-4888-b477-1edc9de68eb5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### JIT OPTIMIZED FUNCTIONS ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94871a82-e15d-431d-9090-cb7bdcc5548a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transition_model_to_numpy(transition_model, obs_space, action_space):\n",
    "    transition_array = np.zeros((obs_space, action_space, obs_space, 3)) \n",
    "    for state in range(1, obs_space):\n",
    "        for action in range(action_space):\n",
    "            for next_state in range(1, obs_space):\n",
    "                transition_info = transition_model[state][action].get(next_state, {'count': 0, 'probability': 0, 'reward': 0})\n",
    "                transition_array[state, action, next_state] = [transition_info.get('count', 0), transition_info.get('probability', 0), transition_info.get('reward', 0)]\n",
    "    return transition_array\n",
    "\n",
    "\n",
    "\n",
    "@nb.jit\n",
    "def value_iteration_gpu(iterations, obs_space, action_space, transition_model, discount_factor, q_values):\n",
    "    epsilon = 0.0000001\n",
    "    delta = 1.0  \n",
    "    cl = 0\n",
    "    while iterations > 0 and delta > epsilon: # check for iteration (MBUS) or convergence\n",
    "        delta = 0\n",
    "        \n",
    "        for state in range(obs_space):\n",
    "            tmp = -1e100\n",
    "            prev = q_values[state].copy() * 1.0\n",
    "            for action in range(action_space):\n",
    "                t = 0\n",
    "                for next_state in range(obs_space):\n",
    "                    count, probability, reward = transition_model[state, action, next_state]\n",
    "                    t += probability * (reward + discount_factor * np.max(q_values[next_state]))\n",
    "                q_values[state, action] = t\n",
    "            \n",
    "                # Calculate delta\n",
    "            Vs = np.max(np.abs(prev - q_values[state]))\n",
    "            delta = max(delta, Vs)\n",
    "        \n",
    "        iterations -= 1  # Decrease the number of iterations\n",
    "        cl += 1\n",
    "    return q_values\n",
    "\n",
    "@nb.jit\n",
    "def softmax(x, temperature):\n",
    "        e_x = np.exp(x / temperature)\n",
    "        return e_x / np.sum(e_x)\n",
    "\n",
    "# Prioritized sweeping test\n",
    "@nb.jit\n",
    "def prioritized_sweeping(state, transition_model, q_values, mbus, T_MB, observation_space, action_space, discount_factor):\n",
    "    \n",
    "    H = [0] * len(range(observation_space))\n",
    "    V = [0] * len(range(observation_space))\n",
    "    \n",
    "    for s in range(observation_space):\n",
    "        H[s] = 0\n",
    "        V[s] = 0\n",
    "    \n",
    "    steps = 0\n",
    "\n",
    "    all_states = list(range(observation_space))\n",
    "    while steps < mbus:\n",
    "        steps += 1\n",
    "\n",
    "        state_probs = softmax(np.array([H[s] for s in range(observation_space)]), T_MB)\n",
    "        \n",
    "        s_idx = all_states[np.searchsorted(np.cumsum(state_probs), np.random.random(), side=\"right\")]\n",
    "        s = s_idx \n",
    "        \n",
    "\n",
    "        for action in range(action_space):\n",
    "            t = 0\n",
    "            for next_state in range(observation_space):\n",
    "                _, probability, reward = transition_model[s, action, next_state]\n",
    "                t += probability * (reward + discount_factor * np.max(q_values[next_state]))\n",
    "            q_values[s, action] = t\n",
    "        \n",
    "        # Update V value\n",
    "        M = -1e100\n",
    "        for a in range(action_space):\n",
    "            M = max(q_values[s, a], M)\n",
    "        delta = abs(V[s] - M)\n",
    "        V[s] = M\n",
    "        # Update H values\n",
    "        for s_prime in range(observation_space):\n",
    "            tmp = 0\n",
    "            for a in range(action_space):\n",
    "                _, probability, _ = transition_model[s_prime, a, s]\n",
    "                tmp = max(tmp, probability)\n",
    "            h_s_prime = delta * tmp\n",
    "            if s_prime != s:\n",
    "                H[s_prime] = max(h_s_prime, H[s_prime])\n",
    "            else:\n",
    "                H[s] = h_s_prime\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de86d2-637a-4310-8d84-d8f52b9e4404",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### IMPLEMENTAZIONE AGENTE MB\n",
    "L'agente MB Ã¨ realizzato utilizzando la value iteration per ricavare le politiche ottimali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b51e49-a670-409b-b971-296ab0ff74f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MBLearningAgent:\n",
    "    def __init__(self, learning_rate: float, initial_epsilon: float, epsilon_decay: float, final_epsilon: float, discount_factor: float = 0.9, mbus = 50 ,theta=0.01):\n",
    "        \n",
    "        # MB params\n",
    "        self.q_values = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        \n",
    "        self.mbus = mbus\n",
    "        self.transition_model = {} \n",
    "        self.initialize_transition_model()\n",
    "        \n",
    "        self.training_error = []\n",
    "        \n",
    "        self.theta = theta \n",
    "        self.upd = 0\n",
    "\n",
    "        # Parameters\n",
    "        self.T_MB = 1.0  \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon: # epsilon greedy \n",
    "            return env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            return np.argmax(self.q_values[state]) # Exploit learned values\n",
    "\n",
    "    def value_iteration(self, iter): # no convergence old\n",
    "        epsilon = 0.1\n",
    "        for _ in range(1, iter):\n",
    "            delta = 0\n",
    "            for state in range(1, env.observation_space.n):\n",
    "                for action in range(0,  env.action_space.n):\n",
    "                    t = 0\n",
    "                    for next_state in self.transition_model[state][action]:\n",
    "                        probability = self.transition_model[state][action][next_state]['probability']\n",
    "                        reward = self.transition_model[state][action][next_state]['reward']\n",
    "                        t += probability * (reward + self.discount_factor * np.max(self.q_values[next_state]))\n",
    "                    self.q_values[state][action] = t\n",
    "                     \n",
    "                Vs = abs(self.q_values[state][action] - np.max(self.q_values[state]))\n",
    "                delta = max(delta, Vs)\n",
    "                \n",
    "            if delta < epsilon and delta > 0: # convergenza\n",
    "                break\n",
    "                    \n",
    "    def initialize_transition_model(self):\n",
    "        for state in range(env.observation_space.n):\n",
    "            self.transition_model[state] = {}\n",
    "            for action in range(env.action_space.n):\n",
    "                self.transition_model[state][action] = {}\n",
    "                for next_state in range(env.observation_space.n):\n",
    "                    self.transition_model[state][action][next_state] = {'count': 0, 'probability': 0, 'reward': 0}\n",
    "    \n",
    "    # Facade for MF compability\n",
    "    def update(self, obs, action, reward, terminated, next_obs):\n",
    "        self.update_transition_model(obs, action, next_obs, reward)\n",
    "        \n",
    "    def update_transition_model(self, state, action, next_state, reward):\n",
    "        self.transition_model[state][action][next_state]['count'] += 1\n",
    "        self.transition_model[state][action][next_state]['reward'] = reward\n",
    "        self.calculate_transition_probabilities()\n",
    "\n",
    "        # For changing calculation method remove the comment and comment the other\n",
    "        #1. prioritized sweeping\n",
    "        transition_array = transition_model_to_numpy(self.transition_model, env.observation_space.n, env.action_space.n)\n",
    "        self.q_values = prioritized_sweeping(state, transition_array, self.q_values, self.mbus, self.T_MB, env.observation_space.n, env.action_space.n, self.discount_factor)\n",
    "        \n",
    "        #2. value iteration JIT optimized (FAST)\n",
    "        #transition_array = transition_model_to_numpy(self.transition_model, env.observation_space.n, env.action_space.n)\n",
    "        #self.q_values = value_iteration_gpu(self.mbus, env.observation_space.n, env.action_space.n, transition_array, self.discount_factor, self.q_values)\n",
    "        \n",
    "        #3. value iteration (SLOW)\n",
    "        #self.value_iteration(self.mbus)\n",
    "        \n",
    "    def calculate_transition_probabilities(self):\n",
    "        for state in range(env.observation_space.n):\n",
    "            for action in range(env.action_space.n):\n",
    "                total_count = sum(self.transition_model[state][action][next_state]['count'] for next_state in self.transition_model[state][action])\n",
    "                for next_state in self.transition_model[state][action]:\n",
    "                    \n",
    "                    c = self.transition_model[state][action][next_state]['count']\n",
    "                    if total_count == 0:\n",
    "                        total_count = 1\n",
    "                    self.transition_model[state][action][next_state]['probability'] = c / total_count\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "       self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50380e52-6c37-4f0b-a98b-96065632910f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## AGENTE MX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f4bc3-0866-4d73-ad57-092bd18fb92e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### IMPLEMENTAZIONE AGENTE MX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09fd1a08-344d-41f7-9bd3-4c3473d3286b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MXLearningAgent:\n",
    "    def __init__(self, lr, ie, ed, fe, Beta, Epsilon, MBUS):\n",
    "        self.MF_agent = MFLearningAgent(\n",
    "                            learning_rate = lr,\n",
    "                            initial_epsilon = ie,\n",
    "                            epsilon_decay = ed,\n",
    "                            final_epsilon = fe,\n",
    "                        )\n",
    "        self.MB_agent = MBLearningAgent(\n",
    "                            learning_rate = lr,\n",
    "                            initial_epsilon = ie,\n",
    "                            epsilon_decay = ed,\n",
    "                            final_epsilon = fe,\n",
    "                            mbus = MBUS\n",
    "                        )\n",
    "        self.beta = Beta # Balance model based and model free\n",
    "        self.epsilon = Epsilon\n",
    "        self.q_valuesMX = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        if random.uniform(0, 1) > self.epsilon:\n",
    "            if len(set(self.q_valuesMX[state])) == 1:\n",
    "                return env.action_space.sample() \n",
    "            else:\n",
    "                return np.argmax(self.q_valuesMX[state]) \n",
    "        else:\n",
    "            return env.action_space.sample()\n",
    "            \n",
    "    def update(self, obs, action, reward, terminated, next_obs):\n",
    "        self.MF_agent.update(obs, action, reward, terminated, next_obs)\n",
    "        self.MF_agent.decay_epsilon()\n",
    "        self.MB_agent.update_transition_model(obs, action, next_obs, reward)\n",
    "        self.MB_agent.decay_epsilon()\n",
    "        for s in range(env.observation_space.n):\n",
    "            for a in range(env.action_space.n):\n",
    "                 self.q_valuesMX[s][a] = self.beta * self.MB_agent.q_values[s][a] + (1-self.beta) * self.MF_agent.q_values[s][a]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064a2fdc-37f7-4a76-b556-fb2e04e3c10a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TEST MX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ebfdec-8f17-4fa4-b4cf-3fb8da268adb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "n_episodes = 1\n",
    "\n",
    "ie = 0.4\n",
    "ed = ie / (env.unwrapped.get_iter() / 2)  # reduce the exploration over time\n",
    "fe = 0.1\n",
    "\n",
    "beta = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b8cee31-05c2-4242-bfa0-b7893bac6172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50a289e5f414b9b9860eea7efc305c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\livel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gymnasium\\utils\\passive_env_checker.py:127: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method should be an int or np.int64, actual type: <class 'numpy.int32'>\u001b[0m\n",
      "  logger.warn(f\"{pre} should be an int or np.int64, actual type: {type(obs)}\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m next_obs, agentReward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# update the agent\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43magentMX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magentReward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m     35\u001b[0m state \u001b[38;5;241m=\u001b[39m obs\n",
      "Cell \u001b[1;32mIn[6], line 32\u001b[0m, in \u001b[0;36mMXLearningAgent.update\u001b[1;34m(self, obs, action, reward, terminated, next_obs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMF_agent\u001b[38;5;241m.\u001b[39mupdate(obs, action, reward, terminated, next_obs)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMF_agent\u001b[38;5;241m.\u001b[39mdecay_epsilon()\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMB_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_transition_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMB_agent\u001b[38;5;241m.\u001b[39mdecay_epsilon()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mn):\n",
      "Cell \u001b[1;32mIn[5], line 70\u001b[0m, in \u001b[0;36mMBLearningAgent.update_transition_model\u001b[1;34m(self, state, action, next_state, reward)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_transition_probabilities()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# For changing calculation method remove the comment and comment the other\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m#1. prioritized sweeping\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m transition_array \u001b[38;5;241m=\u001b[39m \u001b[43mtransition_model_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_values \u001b[38;5;241m=\u001b[39m prioritized_sweeping(state, transition_array, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmbus, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT_MB, env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mn, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount_factor)\n",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m, in \u001b[0;36mtransition_model_to_numpy\u001b[1;34m(transition_model, obs_space, action_space)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransition_model_to_numpy\u001b[39m(transition_model, obs_space, action_space):\n\u001b[1;32m----> 2\u001b[0m     transition_array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, obs_space):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(action_space):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SIMULATION = 900\n",
    "\n",
    "pbar = tqdm(total = SIMULATION)\n",
    "\n",
    "#Serialize info\n",
    "envSerializerObject = EnvSerializer()\n",
    "\n",
    "for a in range(SIMULATION):\n",
    "    envSerializerObject.clean_value_at_step()\n",
    "    agentMX = MXLearningAgent(\n",
    "        lr = lr,\n",
    "        ie = ie,\n",
    "        ed = ed,\n",
    "        fe = fe,\n",
    "        Beta = beta,\n",
    "        Epsilon = 0.1,\n",
    "        MBUS = 50,\n",
    "    )\n",
    "    \n",
    "\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    done = False\n",
    "\n",
    "    step = 1\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agentMX.get_action(obs)\n",
    "        next_obs, agentReward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # update the agent\n",
    "        agentMX.update(obs, action, agentReward, terminated, next_obs)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        state = obs\n",
    "        obs = next_obs\n",
    "\n",
    "        recommender = env.unwrapped.get_recommender()\n",
    "        q_valuesRecommender = recommender.get_qValues()\n",
    "        # Cumulative recommender rewards \n",
    "        rewards_recommender = recommender.get_rewards()\n",
    "\n",
    "        selected_arm = recommender.get_arm()\n",
    "        transition_model = agentMX.MB_agent.transition_model\n",
    "        value_at_step_structure = [state, action, agentReward, agentMX.q_valuesMX.copy(), selected_arm, q_valuesRecommender.copy(), rewards_recommender.copy(), transition_model.copy()]\n",
    "\n",
    "        envSerializerObject.add_value_at_step(value_at_step_structure)\n",
    "    envSerializerObject.serialize_data()\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776e607",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
